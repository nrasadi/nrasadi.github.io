---
title: "Gist - Optimizing Segmentation for Decentralized Federated Learning on Tiny Devices"
tags: "Decentralized Federated Learning, Gossip Learning, Communication Efficient, Embedded Systems, Microcontroller, Adaptive Learning Rate, Digital Twins, Scalable Systems"
authors: "Navidreza Asadi*, Halil İbrahim Bengu*, Lars Wulfert*, Hendrik Wöhrle, Wolfgang Kellerer"
type: "c"
place: "ACM MobiCom FLEdge-AI 2025"
date: "2025/11/03"
status: "a"
# award: "Best Poster Award"
# pdf: https://2025.eurosys.org/posters/final/eurosys25posters-final13.pdf
# link: "https://2025.eurosys.org/posters/final/eurosys25posters-final13.pdf"
# bibtex: "
# @article{wavesurfer2025,   
#   title={WaveSurfer - Scheduling Irregular Pulsing Attacks on Microservice Autoscaling},
#   author={Asadi, Navidreza and Ursu, Răzvan-Mihai and Wong, Leon and Kellerer, Wolfgang},
#   conference={ACM SIGCOMM 2025},
#   year={2025},
#   publisher={Association for Computing Machinery},
# }
# "
misc:
    - title: "* Equal Contribution"
---
We introduce Gist, a decentralized federated learning framework for tiny microcontrollers. Rather than considering all model parameters as equally important, we let devices get the "gist" of the updates. Our contribution has three pillars: (1) it segments model parameters by their importance, identifying the most impactful updates; (2) it shares the segments probabilistically, ensuring rapid propagation of important knowledge while maintaining model diversity; and (3) it aggregates updates using a success-based scheme, giving more weight to information from better-performing peers. We implement and validate Gist through various simulation experiments, realistic large-scale emulation, and deployment on a physical cluster of ESP32-S3 microcontrollers. Across three models and two tasks, Gist outperforms existing baselines, achieving higher accuracy and faster convergence, especially in larger networks consisting of hundreds of devices.